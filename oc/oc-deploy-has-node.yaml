apiVersion: v1
kind: Template
metadata:
  name: worker
  annotations:
    description: HDFS cluster worker deploymentconfig
    tags: networking,storage

parameters:
  - name: ID
    value: "1"
    description: Node ID
    required: true
  - name: IMAGE_HDFS
    value: "hdfs"
    description: HAS HDFS Docker image
    required: true
  - name: IMAGE_ALLUXIO
    value: "alluxio"
    description: HAS Alluxio Docker image
    required: true
  - name: IMAGE_SPARK
    value: "spark"
    description: HAS Spark Docker image
    required: true
  - name: ALLUXIO_MEMORY
    value: "4096MB"
  - name: SPARK_MEMORY
    value: "2048MB"
  - name: HDFS_MEMORY
    value: "1024MB"
  - name: HDFS_CONF_FILES
    required: true
  - name: HDFS_CONF_VARS
    required: true
  - name: CORE_SITE_CONF
    required: true
  - name: HDFS_SITE_CONF
    required: true
  - name: HTTPFS_HTTP_PORT
    required: true
  - name: HTTPFS_ADMIN_PORT
    required: true
  - name: ALLUXIO_CONF_FILES
    required: true
  - name: ALLUXIO_CONF_VARS
    required: true
  - name: ALLUXIO_CONF
    required: true
  - name: ALLUXIO_WORKER_MEMORY_SIZE
    required: true
  - name: ALLUXIO_RAM_FOLDER
    required: true
  - name: ALLUXIO_UNDERFS_ADDRESS
    required: true
  - name: SPARK_CONF_FILES
    required: true
  - name: SPARK_CONF_VARS
    required: true
  - name: SPARK_CONF
    required: true
  - name: SPARK_MASTER_WEBUI_PORT
    required: true
  - name: SPARK_WORKER_MEMORY
    required: true
  - name: SPARK_WORKER_PORT
    required: true
  - name: SPARK_WORKER_WEBUI_PORT
    required: true
  - name: SPARK_DAEMON_MEMORY
    required: true
  - name: HADOOP_CONFIG_DIR
    required: true
objects:

  - apiVersion: v1
    kind: Service
    metadata:
      name: worker${ID}
    spec:
      selector:
        app: worker${ID}
      ports:
        - name: dfs-datanode-address
          protocol: TCP
          port: 50010
          targetPort: 50010
        - name: dfs-datanode-http-address
          protocol: TCP
          port: 50075
          targetPort: 50075
        - name: dfs-datanode-ipc-address
          protocol: TCP
          port: 50020
          targetPort: 50020
        - name: alluxio-worker-metadata
          protocol: TCP
          port: 29998
          targetPort: 29998
        - name: alluxio-worker-data-transfer
          protocol: TCP
          port: 29999
          targetPort: 29999
        - name: alluxio-worker-rpc-ui
          protocol: TCP
          port: 30000
          targetPort: 30000
        - name: spark-worker-service
          protocol: TCP
          port: 35000
          targetPort: 35000
        - name: spark-worker-ui
          protocol: TCP
          port: 8081
          targetPort: 8081
        - name: spark-worker-service-bis
          protocol: TCP
          port: 6066
          targetPort: 6066
        - name: spark-blockmanager-port
          protocol: TCP
          port: 51400
          targetPort: 51400

  - apiVersion: v1
    kind: Route
    metadata:
      name: hdfs-worker${ID}
      namespace: has
    spec:
      to:
        kind: Service
        name: worker${ID}
        weight: 100
      port:
        targetPort: dfs-datanode-http-address
      wildcardPolicy: None

  - apiVersion: v1
    kind: Route
    metadata:
      name: spark-worker${ID}
      namespace: has
    spec:
      to:
        kind: Service
        name: worker${ID}
        weight: 100
      port:
        targetPort: spark-worker-ui
      wildcardPolicy: None

  - apiVersion: v1
    kind: Route
    metadata:
      name: alluxio-worker${ID}
      namespace: has
    spec:
      to:
        kind: Service
        name: worker${ID}
        weight: 100
      port:
        targetPort: alluxio-worker-rpc-ui
      wildcardPolicy: None

  - apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: worker${ID}
      namespace: has
      labels:
        app: worker${ID}
        type: "worker"
    spec:
      strategy:
        type: Recreate
      triggers:
        - type: ConfigChange
        - type: ImageChange
          imageChangeParams:
            automatic: true
            containerNames:
              - worker
            from:
              kind: ImageStreamTag
              namespace: has
              name: 'hdfs:latest'
      replicas: 1
      test: false
      selector:
        app: worker${ID}
        deploymentconfig: worker${ID}
      template:
        metadata:
          labels:
            app: worker${ID}
            deploymentconfig: worker${ID}
            type: "worker"
          annotations:
            scheduler.alpha.kubernetes.io/affinity: >
                {
                  "podAntiAffinity": {
                    "requiredDuringSchedulingIgnoredDuringExecution": [{
                      "podAffinityTerm": {
                        "labelSelector": {
                          "matchExpressions": [{
                            "key": "type",
                            "operator": "In",
                            "values": ["master", "driver", "worker", "aux"]
                          }]
                        }
                      },
                      "topologyKey": "kubernetes.io/hostname"
                    }]
                  }
                }
        spec:
          hostname: worker${ID}
          volumes:
            - name: hdfs-worker-storage-${ID}
              emptyDir: {}
            - name: alluxio-worker-storage-${ID}
              emptyDir: {}
            - name: spark-worker-storage-${ID}
              emptyDir: {}
            - name: dshm
              emptyDir:
              medium: Memory
          containers:
            - name: hdfs-worker${ID}
              image: ${IMAGE_HDFS}
              resources: {}
              environment:
                - CONF_FILES="${HDFS_CONF_FILES}"
                - CONF_VARS="${HDFS_CONF_VARS}"
                - CORE_SITE_CONF="${CORE_SITE_CONF}"
                - HDFS_SITE_CONF="${HDFS_SITE_CONF}"
                - HTTPFS_HTTP_PORT="${HTTPFS_HTTP_PORT}"
                - HTTPFS_ADMIN_PORT="${HTTPFS_ADMIN_PORT}"
              args:
                - datanode
                - start
                - hdfs-namenode
              volumeMounts:
                - name: hdfs-worker-storage-${ID}
                  mountPath: /data
              terminationMessagePath: /dev/termination-log
              imagePullPolicy: Always
            - name: alluxio-worker-${ID}
              image: ${IMAGE_ALLUXIO}
              resources: {}
              environment:
                - CONF_FILES="${ALLUXIO_CONF_FILES}"
                - CONF_VARS="${ALLUXIO_CONF_VARS}"
                - CORE_SITE_CONF="${CORE_SITE_CONF}"
                - HDFS_SITE_CONF="${HDFS_SITE_CONF}"
                - ALLUXIO_CONF="${ALLUXIO_CONF}"
                - ALLUXIO_WORKER_MEMORY_SIZE="${ALLUXIO_WORKER_MEMORY_SIZE}"
                - ALLUXIO_RAM_FOLDER="${ALLUXIO_RAM_FOLDER}"
                - ALLUXIO_UNDERFS_ADDRESS="${ALLUXIO_UNDERFS_ADDRESS}"
                - HADOOP_CONFIG_DIR="${HADOOP_CONFIG_DIR}"
              args:
                - slave
                - start
                - alluxio-master
              volumeMounts:
                - name: alluxio-worker-storage-${ID}
                  mountPath: /data
                - name: dshm
                  mountPath: /dev/shm
              terminationMessagePath: /dev/termination-log
              imagePullPolicy: Always
              env:
                - name: ALLUXIO_WORKER_MEMORY_SIZE
                  value: ${ALLUXIO_MEMORY}
            - name: spark-worker-${ID}
              image: ${IMAGE_SPARK}
              resources: {}
              environment:
                - CONF_FILES="${SPARK_CONF_FILES}"
                - CONF_VARS="${SPARK_CONF_VARS}"
                - CORE_SITE_CONF="${CORE_SITE_CONF}"
                - HDFS_SITE_CONF="${HDFS_SITE_CONF}"
                - SPARK_CONF="${SPARK_CONF}"
                - SPARK_MASTER_WEBUI_PORT="${SPARK_MASTER_WEBUI_PORT}"
                - SPARK_WORKER_MEMORY="${SPARK_WORKER_MEMORY}"
                - SPARK_WORKER_PORT="${SPARK_WORKER_PORT}"
                - SPARK_WORKER_WEBUI_PORT="${SPARK_WORKER_WEBUI_PORT}"
                - SPARK_DAEMON_MEMORY="${SPARK_DAEMON_MEMORY}"
                - HADOOP_CONFIG_DIR="${HADOOP_CONFIG_DIR}"
              args:
                - slave
                - start
                - spark-master
              volumeMounts:
                - name: spark-worker-storage-${ID}
                  mountPath: /data
              terminationMessagePath: /dev/termination-log
              imagePullPolicy: Always
              env:
                - name: SPARK_WORKER_MEMORY
                  value: ${SPARK_MEMORY}
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
          dnsPolicy: ClusterFirst
          securityContext: {}
